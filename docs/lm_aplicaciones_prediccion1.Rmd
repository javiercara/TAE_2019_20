---
title: "Aplicaciones del modelo de regresión lineal: cálculo de predicciones"
date: "`r format(Sys.time(), '%d %B %Y')`"
output: 
  html_document:
    number_sections: true
    toc: true
    toc_float: true
    theme: readable
    highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Predicción del valor medio

Sea el modelo de regresión

$$
y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + \cdots + \beta_k x_{ki} + \epsilon_i, \ i = 1,2,\cdots,n
$$

Que se también se puede escribir como:

$$
y_i = 
\begin{bmatrix}
1 & x_{1i} & x_{2i} & \cdots & x_{ki}
\end{bmatrix}
\begin{bmatrix}
\beta_0 \\ \beta_1 \\ \beta_2 \\ \cdots \\ \beta_k
\end{bmatrix}
+ \epsilon_i
=
x_i^T \beta + \epsilon_i, \ i = 1,2,\cdots,n
$$

Utilizando parámetros estimados y residuos:

$$
y_i = 
\begin{bmatrix}
1 & x_{1i} & x_{2i} & \cdots & x_{ki}
\end{bmatrix}
\begin{bmatrix}
\hat \beta_0 \\ \hat \beta_1 \\ \hat \beta_2 \\ \cdots \\ \hat \beta_k
\end{bmatrix}
+ \epsilon_i
=
x_i^T \hat \beta + e_i, \ i = 1,2,\cdots,n
$$

Dado un valor "nuevo" de los regresores $x_p^T = [1 \ x_{1p} \ x_{2p} \ \cdots \ x_{kp}]$, se define la predicción del valor medio $y$ en $x_p$ como:

$$
\hat y_p = x_p^T \hat \beta
$$

Es un estimador centrado del valor medio de $y_p$, $E[y_p]$:

$$
E[\hat y_p] = E[x_p^T \hat \beta] = x_p^T \beta
$$

Y con varianza:

$$
Var[\hat y_p] = Var[x_p^T \hat \beta] = x_p^T Var[\hat \beta] x_p = \sigma^2 x_p^T (X^TX)^{-1} x_p
$$

Por tanto

$$
\hat y_p \sim N(x_p^T \beta, \sigma^2 x_p^T (X^TX)^{-1} x_p) \Rightarrow \frac{\hat y_p - x_p^T \beta}{\hat s_R\sqrt{x_p^T (X^TX)^{-1} x_p}} \sim t_{n-k-1}
$$

Finalmente, el intervalo de confianza para $x_p^T \beta$ es:

$$
x_p^T \hat \beta - t_{\alpha/2} \hat s_R\sqrt{x_p^T (X^TX)^{-1} x_p} \leq (x_p^T \beta) \leq x_p^T \hat \beta + t_{\alpha/2} \hat s_R\sqrt{x_p^T (X^TX)^{-1} x_p}
$$


# Intervalo de predicción

El error de predicción que cometemos utilizando la predicción del nivel medio es:

$$
e_{p} = y_p - x_p^T \hat \beta
$$

La distribución de $e_p$ es:

$$
\boxed{ e_p \sim N(0, \sigma^2(1 + x_p^T (X^TX)^{-1} x_p)) }
$$

ya que:

$$
E[e_p] = E[y_p] - x_p^T E[\hat \beta] = x_p^T \beta - x_p^T \beta = 0
$$

$$
Var[e_p] = Var[y_p] + Var[x_p^T \hat \beta] = \sigma^2 + x_p^T Var[\hat \beta] x_p= \sigma^2 (1 + x_p^T (X^TX)^{-1} x_p)
$$

Por tanto:

$$
\frac{e_p}{\hat s_R\sqrt{1 + x_p^T (X^TX)^{-1} x_p}} \sim t_{n-k-1}
$$

Y por tanto:

$$
P \left( - t_{\alpha/2} \hat s_R\sqrt{1 + x_p^T (X^TX)^{-1} x_p} \leq e_p \leq t_{\alpha/2} \hat s_R\sqrt{1 + x_p^T (X^TX)^{-1} x_p} \right) = 1 - \alpha
$$

Finalmente, el intervalo de predicción es:

$$
P \left( x_p^T \hat \beta - t_{\alpha/2} \hat s_R\sqrt{1 + x_p^T (X^TX)^{-1} x_p} \leq y_p \leq x_p^T \hat \beta + t_{\alpha/2} \hat s_R\sqrt{1 + x_p^T (X^TX)^{-1} x_p} \right) = 1 - \alpha
$$

# Ejemplo

```{r}
d = read.csv("datos/kidiq.csv")
str(d)
```

Las variables son las siguientes:

- **kid_score**: puntuacion de un test cognitivo en niños de 3-4 años
- **mom_hs**:
    - mom_hs = 1: las madres han terminado high school
    - mom_hs = 0: las madres no terminaron high school
- **mom_iq**: puntuación de la madre en un test IQ
- **mom_work**:
    - mom_work = 1: la madre no trabajó en los primeros tres años del niño
    - mom_work = 2: la madre trabajó en el 2do o tercer año
    - mom_work = 3: la madre trabajó a tiempo parcial el primer año
    - mom_work = 4: la madre trabajó a tiempo completo el primer año
- **mom_age**: edad de la madre


```{r}
d$mom_hs = factor(d$mom_hs, labels = c("no","yes"))
d$mom_work = factor(d$mom_work)
```

## Predicción en un modelo de regresión simple

```{r}
m = lm(kid_score ~ mom_iq, data = d)
plot(d$mom_iq, d$kid_score, pch = 19)
abline(m, col = "red", lwd = 1)
```

### Prediccion del valor medio

```{r}
xp = data.frame(mom_iq = 130)
(yp_medio = predict(m, newdata = xp, interval = "confidence", level = 0.95))
```

```{r}
plot(d$mom_iq, d$kid_score, pch = 19)
abline(m, col = "red", lwd = 1)
points(xp$mom_iq, yp_medio[1], col = "red", pch = 19) # prediccion puntual
points(xp$mom_iq, yp_medio[2], col = "red", pch = 19) # limite inferior int. conf.
points(xp$mom_iq, yp_medio[3], col = "red", pch = 19) # limite superior int. conf.
```

### Intervalo de prediccion


```{r}
(yp = predict(m, newdata = xp, interval = "prediction", level = 0.95))
```

```{r}
plot(d$mom_iq, d$kid_score, pch = 19)
abline(m, col = "red", lwd = 1)
points(xp$mom_iq, yp_medio[1], col = "red", pch = 19) # prediccion puntual
points(xp$mom_iq, yp_medio[2], col = "red", pch = 19) # limite inferior int. conf.
points(xp$mom_iq, yp_medio[3], col = "red", pch = 19) # limite superior int. conf.
points(xp$mom_iq, yp[2], col = "green", pch = 19) # limite inferior int. pred.
points(xp$mom_iq, yp[3], col = "green", pch = 19) # limite superior int. pred.
```


## Predicción en un modelo de regresión múltiple

Vamos a predecir:

- mom_iq = 130
- mon_hs = no
- mom_age = 25
- mom_work = 3

```{r}
m2 = lm(kid_score ~ mom_iq + mom_hs + mom_age + mom_work, data = d)
```

### Prediccion del valor medio

```{r}
xp = data.frame(mom_iq = 130, mom_hs = "no", mom_age = 25, mom_work = "3")
(yp_medio = predict(m2, newdata = xp, interval = "confidence", level = 0.95))
```

### Intervalo de prediccion

```{r}
(yp = predict(m2, newdata = xp, interval = "prediction", level = 0.95))
```

